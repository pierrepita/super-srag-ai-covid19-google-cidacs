{"cells": [{"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "# # Caso ja n\u00e3o tenha instalado, instale os seguintes pacotes:\n\n# !pip install cmake\n# !pip install xgboost\n# !pip install sklearn\n# !pip install sklearn-deap\n# # (para usar o EvolutionarySearch >> https://github.com/rsteca/sklearn-deap)\n# !pip install scikit-plot"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "64\n"}], "source": "import multiprocessing \n \nprint(multiprocessing.cpu_count()) "}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "import pandas as pd\nimport pyspark.sql.functions as F\nfrom datetime import datetime\nfrom pyspark.sql.types import *\nfrom pyspark import StorageLevel\n\nimport numpy as np\npd.set_option(\"display.max_rows\", 1000)\npd.set_option(\"display.max_columns\", 1000)\npd.set_option(\"mode.chained_assignment\", None)"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "import xgboost as xgb #XGBClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport matplotlib.pylab as plt\n\nfrom sklearn import metrics\nfrom evolutionary_search import EvolutionaryAlgorithmSearchCV\nimport scikitplot as skplt"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "import sklearn\nimport scikitplot as skplt\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_score"}, {"cell_type": "markdown", "metadata": {}, "source": "<hr />\n<hr />\n<hr />"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "# undersamp_col = ['02-KMODES', '03-STRSAMP-AG', '04-STRSAMP-EW'] \n# dfs = ['ds-1', 'ds-2', 'ds-3']\n# cols_sets = ['cols_set_1', 'cols_set_2', 'cols_set_3']\n\nundersamp_col = ['02-KMODES']\ndfs = ['ds-1']\ncols_sets = ['cols_set_3']"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "There is 1 set of params.\n"}], "source": "# lists of params\nmodel_MaxEstimators = [50] \nmodel_maxDepth = [10] \n\nlist_of_param_dicts = []\n\nfor maxIter in model_MaxEstimators:\n    for maxDepth in model_maxDepth: \n        params_dict = {}\n        params_dict['MaxEstimators'] = maxIter\n        params_dict['maxDepth'] = maxDepth\n        list_of_param_dicts.append(params_dict)\n\nprint(\"There is {} set of params.\".format(len(list_of_param_dicts)))\n# list_of_param_dicts"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "prefix = 'gs://ai-covid19-datalake/trusted/experiment_map/'"}, {"cell_type": "markdown", "metadata": {}, "source": "<hr />\n<hr />\n<hr />"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "# filename = 'gs://ai-covid19-datalake/trusted/experiment_map/02-KMODES/ds-1/cols_set_1/experiment0.parquet'\n# df = spark.read.parquet(filename).sample(0.3)"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "# df = df.toPandas()"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": "# params_dict = {'MaxEstimators': 10,\n#                'maxDepth': 3}\n# cols = 'cols_set_1'\n# experiment_filter = 'ds-1'\n# undersampling_method = '03-STRSAMP-AG', \n# experiment_id = 0"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": "# model = run_xgboost(df, params_dict, cols, filename, experiment_filter, undersampling_method, experiment_id)"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "# print('finished')"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "# model['model_time_exec']"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "# model['model_AUC_PR']"}, {"cell_type": "markdown", "metadata": {}, "source": "<hr />\n<hr />\n<hr />"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": "# Ref: https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python\ndef encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    res = res.drop([feature_to_encode], axis=1)\n    return(res) \n\n\ndef run_xgboost(exp_df, params_dict, cols, filename, experiment_filter, \n            undersampling_method, experiment_id):\n    \n    import time\n    start_time = time.time()\n    \n    n_covid = len(exp_df[exp_df['CLASSI_FIN']==1.0])\n    n_not_covid = len(exp_df[exp_df['CLASSI_FIN']==0.0])\n    \n    id_cols = ['NU_NOTIFIC', 'CLASSI_FIN']\n    \n    for column in exp_df.columns: \n        exp_df[column] = exp_df[column].astype(\"category\")\n        if column != \"CLASSI_FIN\":\n            exp_df = encode_and_bind(exp_df, column)\n            \n    # Sele\u00e7\u00e3o das vari\u00e1veis que ser\u00e3o submetidas ao modelo\n    x = exp_df.drop(\"CLASSI_FIN\", axis=1)\n    y = exp_df.CLASSI_FIN\n    \n    \n    X_train, X_test, y_train, y_test = train_test_split(x, \n                                                        y, \n                                                        test_size=0.3, \n                                                        random_state=2021)\n\n    # Gerando o modelo\n    model = xgb.XGBClassifier(objective=\"binary:logistic\",\n                              n_jobs = 30,\n                              colsample_bytree = 0.3,\n                              learning_rate=0.1, \n                              max_depth= params_dict['maxDepth'],\n                              n_estimators = params_dict['MaxEstimators'],\n                              min_child_weight = 1,\n                              subsample=0.5, \n                              scale_pos_weight=2, \n                              eval_metric=\"error\", \n                              booster='dart')\n\n    model.fit(X_train,y_train)\n    # Predizendo\n    pred = model.predict(X_test)\n    \n    \n    # validation \n    fpr, tpr, thresholds_auc_roc = metrics.roc_curve(y_test, pred)\n    auc_ROC = metrics.auc(fpr, tpr)\n    \n    precision, recall, thresholds = metrics.precision_recall_curve(y_test, pred)\n    aupr_ROC = metrics.auc(recall, precision)\n\n    de_para = {1.0: 'covid', 0.0: 'nao_covid'}\n    y_test = y_test.replace(de_para)\n    pred = pd.Series(pred).replace(de_para)\n\n    report = metrics.classification_report(y_test,pred,output_dict=True)\n    conf_matrix = metrics.confusion_matrix(y_test, pred)\n    \n    \n    # Gerando os metadados\n    result_dict = {}\n\n    result_dict['experiment_filter'] = experiment_filter\n    result_dict['undersampling_method'] = undersampling_method\n    result_dict['filename'] = filename\n    result_dict['experiment_id'] = experiment_id\n    result_dict['n_covid'] = n_covid\n    result_dict['n_not_covid'] = n_not_covid\n    result_dict['model_name'] = 'XGBoost'\n    result_dict['params'] = params_dict\n    result_dict['model_AUC_ROC'] = auc_ROC\n    result_dict['model_AUC_PR'] = aupr_ROC\n    result_dict['model_covid_precision'] = report['covid']['precision']\n    result_dict['model_covid_recall'] = report['covid']['recall']\n    result_dict['model_covid_f1'] = report['covid']['f1-score']\n    result_dict['model_not_covid_precision'] = report['nao_covid']['precision']\n    result_dict['model_not_covid_recall'] = report['nao_covid']['recall']\n    result_dict['model_not_covid_f1'] = report['nao_covid']['f1-score']\n    result_dict['model_avg_precision'] = report['macro avg']['precision']\n    result_dict['model_avg_recall'] = report['macro avg']['recall']\n    result_dict['model_avg_f1'] = report['macro avg']['f1-score']\n    result_dict['model_avg_acc'] = report['accuracy']\n    result_dict['model_TP'] = conf_matrix[0][0]\n    result_dict['model_TN'] = conf_matrix[1][1]\n    result_dict['model_FN'] = conf_matrix[0][1]\n    result_dict['model_FP'] = conf_matrix[1][0]\n    result_dict['model_time_exec'] = time.time() - start_time\n    result_dict['model_col_set'] = cols\n    \n    return result_dict"}, {"cell_type": "markdown", "metadata": {}, "source": "<hr />\n<hr />\n<hr />"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "experiments = []"}, {"cell_type": "markdown", "metadata": {}, "source": "### Datasets:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "read gs://ai-covid19-datalake/trusted/experiment_map/02-KMODES/ds-1/cols_set_3/experiment0.parquet\n"}, {"name": "stderr", "output_type": "stream", "text": "/opt/conda/anaconda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"}, {"name": "stdout", "output_type": "stream", "text": "Parameters ==> {'MaxEstimators': 50, 'maxDepth': 10}\n Results: \n AUC_PR: 0.9368176488063367 \n Precision: 0.9262476181826504 \n Time: 6478.095958471298\n=========================== \n\nread gs://ai-covid19-datalake/trusted/experiment_map/02-KMODES/ds-1/cols_set_3/experiment1.parquet\n"}, {"name": "stderr", "output_type": "stream", "text": "/opt/conda/anaconda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"}], "source": "for uc in undersamp_col: \n    for ds in dfs:\n        for col_set in cols_sets:\n            for params_dict in list_of_param_dicts: \n                for id_exp in range(5):\n                    filename = prefix + uc + '/' + ds + '/' + col_set + '/' + 'experiment' + str(id_exp) + '.parquet'\n                    exp_dataframe = spark.read.parquet(filename)\n                    exp_dataframe = exp_dataframe.toPandas()\n                    print('read {}'.format(filename))\n                    \n                    undersampling_method = uc\n                    experiment_filter = ds\n                    experiment_id = id_exp\n\n                    try:                     \n                        model = run_xgboost(exp_dataframe, params_dict, col_set, filename, experiment_filter, undersampling_method, experiment_id)\n                        experiments.append(model)\n\n                        print(\"Parameters ==> {}\\n Results: \\n AUC_PR: {} \\n Precision: {} \\n Time: {}\".format(str(params_dict), str(model['model_AUC_PR']), str(model['model_avg_precision']), str(model['model_time_exec'])))\n                        print('=========================== \\n')\n                    except:\n                        print('=========== W A R N I N G =========== \\n')\n                        print('Something wrong with the exp: {}, {}, {}'.format(filename, params_dict, col_set))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for i in range(len(experiments)):\n    for d in list(experiments[i].keys()):\n        experiments[i][d] = str(experiments[i][d])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# experiments"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "cols = ['experiment_filter', 'undersampling_method', 'filename', 'experiment_id', 'n_covid', 'n_not_covid', 'model_name', 'params', 'model_AUC_ROC', 'model_AUC_PR', 'model_covid_precision', 'model_covid_recall', 'model_covid_f1', 'model_not_covid_precision', 'model_not_covid_recall', 'model_not_covid_f1', 'model_avg_precision', 'model_avg_recall', 'model_avg_f1', 'model_avg_acc', 'model_TP', 'model_TN', 'model_FN', 'model_FP', 'model_time_exec', 'model_col_set']"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "intermed_results = spark.createDataFrame(data=experiments).select(cols)\nintermed_results.toPandas()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "intermed_results.write.parquet('gs://ai-covid19-datalake/trusted/intermed_results/KMODES/XGBOOST_experiments-kmodes-ds1-cs3.parquet', mode='overwrite')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "print('finished')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "intermed_results.show()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}